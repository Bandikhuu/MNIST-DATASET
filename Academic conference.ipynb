{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Шаардлагатай сангуудыг суулгаж буй хэсэг\n",
    "import numpy as np\n",
    "import array\n",
    "from keras.datasets import mnist\n",
    "import keras.utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "# x = float('nan')\n",
    "# math.isnan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label_data_ mean: 6.037553e-09\n",
      "train_label_data_ var: 0.9999998\n",
      "train_label_data_ std: 0.9999999\n"
     ]
    }
   ],
   "source": [
    "# MNIST DATASET буюу 60000 ширхэг сургалтын 0-9 хооронд бичсэн гар бичмэлийн өгөгдөл болон зөв хариу нь өгөдсөн ба\n",
    "# шалгалтын өгөгдөл нь 10000 ширхэг бөгөөд мөн зөв хариу нь өгөгдсөн.\n",
    "\n",
    "#keras -ын сан ашиглан өгөгдлөө татаж авч буй хэсэг\n",
    "(train_data,train_label),(test_data,test_label) = mnist.load_data()\n",
    "\n",
    "# зөв хариуг нь encoding хийж буй хэсэг\n",
    "\n",
    "train_label_data_ = keras.utils.to_categorical(train_label, 10)\n",
    "test_label_data_ = keras.utils.to_categorical(test_label, 10)\n",
    "\n",
    "# Хар цагаан зураг нь 0-255 хооронд байдаг учир үүнийг 0-1 хооронд болгохын\n",
    "# тулд pixel бүрийг нь 255 хувааж буй хэсэг\n",
    "\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255\n",
    "\n",
    "# Энд сургалтын өгөгдөл болон шалгалтын өгөгдлийн хэмжээг (60000,748) болгож \n",
    "# байна. Уг өгөгдөл нь эхлээд (60000,28,28) гэсэн хэмжээтэй орж ирж байгаа.\n",
    "\n",
    "train_data_data_ = train_data.reshape(60000,784)\n",
    "test_data_data_ = test_data.reshape(10000,784)\n",
    "\n",
    "# 1. mean = 0, var = 1 baihaar data normalize hiij baigaa heseg\n",
    "\n",
    "train_label_data = (train_label_data_ - np.mean(train_label_data_))/np.std(train_label_data_)\n",
    "test_label_data = (test_label_data_ - np.mean(test_label_data_))/np.std(test_label_data_)\n",
    "train_data_data = (train_data_data_ - np.mean(train_data_data_))/np.std(train_data_data_)\n",
    "test_data_data = (test_data_data_ - np.mean(test_data_data_))/np.std(test_data_data_)\n",
    "\n",
    "print('train_label_data_ mean:',np.mean(train_label_data))\n",
    "print('train_label_data_ var:',np.var(train_label_data))\n",
    "print('train_label_data_ std:',np.std(train_label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_label shape:\",train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,epoch_size,batch_size,input_size,hidden_size,output_size,learn_rate):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        \n",
    "        self.epoch = epoch_size\n",
    "        \n",
    "        self.batch = batch_size\n",
    "        \n",
    "        self.input = input_size\n",
    "        \n",
    "        self.hidden = hidden_size\n",
    "        \n",
    "        self.output = output_size\n",
    "        \n",
    "        self.lr = learn_rate\n",
    "        \n",
    "        self.w1 = np.random.normal(loc = 0,scale = 0.1,size=(self.input, self.hidden))\n",
    "        \n",
    "        self.b1 = np.zeros((1,self.hidden))\n",
    "        \n",
    "        self.w2 = np.random.normal(loc = 0,scale = 0.1,size=(self.hidden, self.output))\n",
    "        \n",
    "        self.b2 = np.zeros((1,self.output))\n",
    "        \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def step(self,x):\n",
    "        return np.heaviside(x,1)\n",
    "    \n",
    "    def softmax(self,k):\n",
    "        \n",
    "        k = k.T\n",
    "        \n",
    "        e_x = np.exp(k - np.max(k))\n",
    "        \n",
    "        soft = e_x / e_x.sum(axis=0)\n",
    "        \n",
    "        return soft.T\n",
    "    \n",
    "    def forward_propagation(self,input_data):\n",
    "        \n",
    "        h = np.dot(input_data,self.w1) + self.b1\n",
    "        \n",
    "        H = self.relu(h)\n",
    "        \n",
    "        z = np.dot(H,self.w2) + self.b2\n",
    "        \n",
    "        y = self.softmax(z)\n",
    "\n",
    "        return y,z,H,h\n",
    "    \n",
    "    def error_formula(self,y,y_hat):\n",
    "        loss = []\n",
    "        for i in range(self.batch):\n",
    "#             print(\"example:\",-np.log(y[i][y_hat[i]]))\n",
    "#             print(\"y shape:\",y.shape)\n",
    "#             print(\"y value:\",y)\n",
    "#             print(\"y_hat shape:\",y_hat.shape)\n",
    "#             print(\"y_hat value:\",y_hat)\n",
    "            correct_logprobs = -np.log(y[i,y_hat[i]])\n",
    "            loss.append(correct_logprobs)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def back_propagation(self,input_data,y_hat,y1_hat):\n",
    "        \n",
    "        y,z,H,h = self.forward_propagation(input_data)\n",
    "        \n",
    "        error = (y - y_hat)\n",
    "        \n",
    "        loss = self.error_formula(y,y1_hat)\n",
    "        \n",
    "        d_w1 = np.dot(input_data.T,self.step(h)*np.dot(self.w2,error.T).T) # (784:16)\n",
    "        \n",
    "        d_b1 = self.step(h)*np.dot(self.w2,error.T).T # (batch:16)\n",
    "        \n",
    "        d_w2 = np.dot(H.T,error) # (16:10)\n",
    "        \n",
    "        d_b2 = error # (batch:10)\n",
    "        \n",
    "        if loss > 0:\n",
    "            self.w1 = self.w1 - d_w1*self.lr\n",
    "            self.b1 = self.b1 - d_b1*self.lr\n",
    "            self.w2 = self.w2 - d_w2*self.lr \n",
    "            self.b2 = self.b2 - d_b2*self.lr\n",
    "        else:\n",
    "            self.w1 = self.w1 + d_w1*self.lr\n",
    "            self.b1 = self.b1 + d_b1*self.lr\n",
    "            self.w2 = self.w2 + d_w2*self.lr \n",
    "            self.b2 = self.b2 + d_b2*self.lr\n",
    "        \n",
    "        return loss,y,self.w1,self.w2,self.b1,self.b2\n",
    "    \n",
    "    def predict(self,test_data,test_label,w1,w2,b1,b2):\n",
    "        \n",
    "        h = np.dot(test_data,w1) + b1\n",
    "        \n",
    "        H = self.relu(h)\n",
    "        \n",
    "        z = np.dot(H,w2) + b2\n",
    "        \n",
    "        y = self.softmax(z)\n",
    "        \n",
    "        loss = self.error_formula(y,test_label)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print()\n",
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (60000, 784)\n",
      "Train accuracy: 13.1%\n",
      "count: 7874\n",
      ">Epoch=0, lrate=0.000001, error=2.6265733664\n",
      "Train accuracy: 38.7%\n",
      "count: 23214\n",
      ">Epoch=1, lrate=0.000001, error=2.0504948393\n",
      "Train accuracy: 59.1%\n",
      "count: 35476\n",
      ">Epoch=2, lrate=0.000001, error=1.7307146846\n",
      "Train accuracy: 69.3%\n",
      "count: 41593\n",
      ">Epoch=3, lrate=0.000001, error=1.5496477995\n",
      "Train accuracy: 73.7%\n",
      "count: 44218\n",
      ">Epoch=4, lrate=0.000001, error=1.4654772847\n",
      "train_error: 1.4654772847320847\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 128\n",
    "epoch_size = 5\n",
    "batch_size = 50\n",
    "output_size = 10\n",
    "learn_rate = 0.000001\n",
    "\n",
    "print(\"train data shape:\",train_data_data.shape)\n",
    "\n",
    "super_w1 = np.zeros((input_size,hidden_size))\n",
    "super_w2 = np.zeros((hidden_size,output_size))\n",
    "super_b1 = np.zeros((batch_size,hidden_size))\n",
    "super_b2 = np.zeros((batch_size,output_size))\n",
    "\n",
    "nn = NeuralNetwork(epoch_size,batch_size,input_size,hidden_size,output_size,learn_rate)\n",
    "error_avg = []\n",
    "acc_count = 0\n",
    "\n",
    "for e in range(epoch_size):\n",
    "    count1 = 0\n",
    "    for i in range(0,len(train_data_data),batch_size):\n",
    "        batch = train_data_data[i:i+batch_size]\n",
    "        \n",
    "        label = train_label_data[i:i+batch_size]\n",
    "        \n",
    "        number_label = train_label[i:i+batch_size]\n",
    "        \n",
    "        loss,y,super_w1,super_w2,super_b1,super_b2 = nn.back_propagation(batch,label,number_label)\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            if y[k][number_label[k]]>0.5:\n",
    "                count1 = count1 + 1\n",
    "        acc_count = acc_count + 1\n",
    "        error_avg.append(loss)\n",
    "#         if i % 10000 == 0:\n",
    "#             print(\"Loss:\",np.mean(error_avg))\n",
    "\n",
    "    train_accuracy = (count1 * 100)/60000\n",
    "    print(\"Train accuracy: {:.1f}%\".format(train_accuracy))\n",
    "    print(\"count:\",count1)\n",
    "    print('>Epoch=%d, lrate=%.6f, error=%.10f'%(e, learn_rate, np.mean(error_avg)))\n",
    "print(\"train_error:\",np.mean(error_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 0\n",
      "count: 707\n",
      "count: 1409\n",
      "count: 2118\n",
      "count: 2840\n",
      "count: 3543\n",
      "count: 4370\n",
      "count: 5186\n",
      "count: 5960\n",
      "count: 6793\n",
      "Prediction accuracy: 75.6%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for j in range(0,len(test_data_data),batch_size):\n",
    "    \n",
    "    batch = test_data_data[j:j+batch_size]\n",
    "    \n",
    "    label = test_label[j:j+batch_size]\n",
    "    \n",
    "    h = np.dot(batch,super_w1) + super_b1\n",
    "    \n",
    "    H = nn.relu(h)\n",
    "    \n",
    "    z = np.dot(H,super_w2) + super_b2\n",
    "    \n",
    "    y = nn.softmax(z)\n",
    "    \n",
    "    if j % 1000 == 0:\n",
    "        print(\"count:\",count)\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i][label[i]] > 0.5:\n",
    "            count = count + 1\n",
    "\n",
    "accuracy = (count * 100)/10000\n",
    "\n",
    "print(\"Prediction accuracy: {:.1f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
